# -*- coding: utf-8 -*-
"""Minor project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19LDgEvNA1gRqddSlwWOcJP_glFZE6gk5
"""

import tensorflow as tf
import numpy as np

# Define the PINN model
class PINN(tf.keras.Model):
    def __init__(self):
        super(PINN, self).__init__()
        # Define neural network layers
        self.dense1 = tf.keras.layers.Dense(64, activation='tanh')
        self.dense2 = tf.keras.layers.Dense(64, activation='tanh')
        self.dense3 = tf.keras.layers.Dense(1, activation=None)  # Output layer for y

    def call(self, t):
        # Forward pass through the neural network
        x = self.dense1(t)
        x = self.dense2(x)
        y = self.dense3(x)  # Output y
        return y

# Define the loss function for the new equation: 1.970639y'' + 6.12y' + 46.97y = 49.015208
def loss_fn(model, t, y):
    with tf.GradientTape() as tape1:
        tape1.watch(t)
        with tf.GradientTape() as tape2:
            tape2.watch(t)
            y_pred = model(t)
        y_prime = tape2.gradient(y_pred, t)  # y'
    y_double_prime = tape1.gradient(y_prime, t)  # y''

    # Compute the residual loss for the new equation
    residual_loss = 1.970639 * y_double_prime + 6.12 * y_prime + 46.97 * y_pred - 49.015208

    # Compute the boundary/initial condition loss (adjust as necessary for your problem)
    boundary_loss = (y_pred[0] - 1.0)**2 + (y_prime[0] - 0.0)**2  # Initial conditions (y(0) = 1, y'(0) = 0)

    return tf.reduce_mean(residual_loss**2) + tf.reduce_mean(boundary_loss)

# Generate training data
t_train = np.linspace(0, 250, num=251)[:, None]  # Time span (0, 250)
y_train = np.zeros_like(t_train)  # Placeholder for y (formerly v)

# Convert NumPy arrays to TensorFlow tensors
t_train_tf = tf.convert_to_tensor(t_train, dtype=tf.float32)
y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)

# Create an instance of the PINN model
model = PINN()

# Define optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# Training loop
epochs = 10000
for epoch in range(epochs):
    with tf.GradientTape() as tape:
        loss = loss_fn(model, t_train_tf, y_train_tf)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    if epoch % 100 == 0:
        print(f'Epoch {epoch}, Loss: {loss.numpy()}')

# Evaluate the trained model
y_pred = model(t_train_tf)  # Get predictions

y_pred



import matplotlib.pyplot as plt

# Convert the TensorFlow tensor y_pred to a NumPy array for plotting
y_pred_np = y_pred.numpy()  # Convert the predictions to NumPy array

# Plot the predicted velocity
plt.figure(figsize=(10, 6))
plt.plot(t_train, y_pred_np, label='Predicted Velocity', color='blue', linewidth=2)
plt.xlabel('Time')
plt.ylabel('Velocity')
plt.title('Predicted Velocity vs. Time')
plt.legend()
plt.grid(True)
plt.show()

import tensorflow as tf
import numpy as np

# Define the PINN model
class PINN(tf.keras.Model):
    def __init__(self):
        super(PINN, self).__init__()
        # Define neural network layers
        self.dense1 = tf.keras.layers.Dense(64, activation='tanh')
        self.dense2 = tf.keras.layers.Dense(64, activation='tanh')
        self.dense3 = tf.keras.layers.Dense(1, activation=None)  # Output layer for y

    def call(self, t):
        # Forward pass through the neural network
        x = self.dense1(t)
        x = self.dense2(x)
        y = self.dense3(x)  # Output y
        return y

# Define the loss function for the new equation: 1.970639y'' + 6.12y' + 46.97y = 49.015208
def loss_fn(model, t, y):
    with tf.GradientTape() as tape1:
        tape1.watch(t)
        with tf.GradientTape() as tape2:
            tape2.watch(t)
            y_pred = model(t)
        y_prime = tape2.gradient(y_pred, t)  # y'
    y_double_prime = tape1.gradient(y_prime, t)  # y''

    # Compute the residual loss for the new equation
    residual_loss = 1.970639 * y_double_prime + 6.12 * y_prime + 46.97 * y_pred - 49.015208

    # Compute the boundary/initial condition loss (adjust as necessary for your problem)
    boundary_loss = (y_pred[0] - 1.0)**2 + (y_prime[0] - 0.0)**2  # Initial conditions (y(0) = 1, y'(0) = 0)

    return tf.reduce_mean(residual_loss**2) + tf.reduce_mean(boundary_loss)

# Generate training data
t_train = np.linspace(0, 250, num=251)[:, None]  # Time span (0, 250)
y_train = np.zeros_like(t_train)  # Placeholder for y (formerly v)

# Convert NumPy arrays to TensorFlow tensors
t_train_tf = tf.convert_to_tensor(t_train, dtype=tf.float32)
y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)

# Create an instance of the PINN model
model = PINN()

# Define optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# Training loop
epochs = 10000
for epoch in range(epochs):
    with tf.GradientTape() as tape:
        loss = loss_fn(model, t_train_tf, y_train_tf)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    if epoch % 100 == 0:
        print(f'Epoch {epoch}, Loss: {loss.numpy()}')

# Evaluate the trained model
y_pred = model(t_train_tf)  # Get predictions

y_pred